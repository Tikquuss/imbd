#### Dependencies

- [Python 3](https://www.python.org/downloads/)
- [NumPy](http://www.numpy.org/)
- [PyTorch](http://pytorch.org/) 
- [torchtext](https://pypi.org/project/torchtext/)
- [transformers](https://pypi.org/project/transformers/)
- [spacy](https://pypi.org/project/spacy/) : after installing spacy, run this in your terminal : `python -m spacy download en` [source](https://github.com/hamelsmu/Seq2Seq_Tutorial/issues/1)

## Abstract

## I. Introduction

## II. Related Work

## III. Background

### III.1 RNN and LSTM
### III.2 CNN
### III.3 Transformer and BERT

## IV. Methodology

## V. Result

## References

### Cross-lingual Language Model Pretraining

[1] G. Lample *, A. Conneau * [*Cross-lingual Language Model Pretraining*](https://arxiv.org/abs/1901.07291) and [facebookresearch/XLM](https://github.com/facebookresearch/XLM)

```
@article{lample2019cross,
  title={Cross-lingual Language Model Pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
```

### Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

[2] Chelsea Finn, Pieter Abbeel, Sergey Levine [*Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks*](https://arxiv.org/abs/1911.02116) and [cbfinn/maml](https://github.com/cbfinn/maml)

```
@article{Chelsea et al.,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Chelsea Finn, Pieter Abbeel, Sergey Levine},
  journal={Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017},
  year={2017}
}
```

## License
See the [LICENSE](LICENSE) file for more details.