{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.model import get_dataset, Trainer, RNN, LSTM, CNN, CNN1d, BERTGRUSentiment\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def get_dataset(seed = 1234, max_vocab_size = 25000, batch_size = 64, include_lengths = True, vectors = \"glove.6B.100d\", unk_init = torch.Tensor.normal_):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    TEXT = data.Field(tokenize = 'spacy', include_lengths = include_lengths)\n",
    "    LABEL = data.LabelField(dtype = torch.float)\n",
    "                 \n",
    "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    train_data, valid_data = train_data.split(random_state = random.seed(seed))\n",
    "    print(f'Number of training examples: {len(train_data)}')\n",
    "    print(f'Number of validation examples: {len(valid_data)}')\n",
    "    print(f'Number of testing examples: {len(test_data)}')\n",
    "    \n",
    "    TEXT.build_vocab(train_data, max_size = max_vocab_size, vectors = vectors, unk_init = unk_init)\n",
    "    LABEL.build_vocab(train_data)\n",
    "    print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "    print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "    \n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "        batch_size = batch_size, sort_within_batch = True, device = device\n",
    "    )\n",
    "    \n",
    "    return {\"TEXT\" : TEXT, \"LABEL\" : LABEL,\n",
    "            \"train_data\" : train_data, \"valid_data\" : valid_data, \"test_data\" : test_data,\n",
    "            \"train_iterator\" : train_iterator, \"valid_iterator\" : valid_iterator, \"test_iterator\" : test_iterator}\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "    \n",
    "    def getID(self):\n",
    "        return 'RNN'\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout_perent = dropout\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional=bidirectional, dropout = dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "    \n",
    "    def getID(self):\n",
    "        return 'LSTM'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,  dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_filters = n_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_percent = dropout\n",
    "        self.pad_idx = pad_idx \n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1, out_channels = n_filters, kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)\n",
    "    \n",
    "    def getID(self):\n",
    "        return 'CNN'\n",
    "    \n",
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_filters = n_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_percent = dropout\n",
    "        self.pad_idx = pad_idx \n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = embedding_dim, out_channels = n_filters, kernel_size = fs)\n",
    "                                    for fs in filter_sizes])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        #embedded = [batch size, emb dim, sent len]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)\n",
    "    \n",
    "    def getID(self):\n",
    "        return 'CNN1d'\n",
    "    \n",
    "    \n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional, batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])       \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def getID(self):\n",
    "        return 'BERT'\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer = None, criterion= None, dump_path = \"\"):\n",
    "        assert any([isinstance(model, className) for className in [RNN, LSTM, CNN, CNN1d, BERTGRUSentiment]]), \"Model type not supported\"\n",
    "        self.model = model\n",
    "        self.count_parameters()\n",
    "        \n",
    "        if isinstance(self.model, RNN) :\n",
    "            \n",
    "            seed = 1234\n",
    "            max_vocab_size = 25000\n",
    "            batch_size = 64\n",
    "            \n",
    "            torch.manual_seed(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "\n",
    "            TEXT = data.Field(tokenize = 'spacy')\n",
    "            LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "            train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "            train_data, valid_data = train_data.split(random_state = random.seed(seed))\n",
    "            print(f'Number of training examples: {len(train_data)}')\n",
    "            print(f'Number of validation examples: {len(valid_data)}')\n",
    "            print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "            TEXT.build_vocab(train_data, max_size = max_vocab_size)\n",
    "            LABEL.build_vocab(train_data)\n",
    "            print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "            print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "            train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "                (train_data, valid_data, test_data), \n",
    "                batch_size = batch_size, device = device\n",
    "            )\n",
    "\n",
    "            self.dataset = {\"TEXT\" : TEXT, \"LABEL\" : LABEL,\n",
    "                    \"train_data\" : train_data, \"valid_data\" : valid_data, \"test_data\" : test_data,\n",
    "                    \"train_iterator\" : train_iterator, \"valid_iterator\" : valid_iterator, \"test_iterator\" : test_iterator}\n",
    "                    \n",
    "        elif isinstance(self.model, LSTM) :\n",
    "            seed = 1234\n",
    "            max_vocab_size = 25000\n",
    "            batch_size = 64\n",
    "            \n",
    "            torch.manual_seed(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "\n",
    "            include_lengths = True\n",
    "            TEXT = data.Field(tokenize = 'spacy', include_lengths = include_lengths)\n",
    "            LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "            train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "            train_data, valid_data = train_data.split(random_state = random.seed(seed))\n",
    "            print(f'Number of training examples: {len(train_data)}')\n",
    "            print(f'Number of validation examples: {len(valid_data)}')\n",
    "            print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "            vectors = \"glove.6B.100d\"\n",
    "            unk_init = torch.Tensor.normal_\n",
    "            TEXT.build_vocab(train_data, max_size = max_vocab_size, vectors = vectors, unk_init = unk_init)\n",
    "            LABEL.build_vocab(train_data)\n",
    "            print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "            print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "    \n",
    "            train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "                (train_data, valid_data, test_data), \n",
    "                batch_size = batch_size, sort_within_batch = True, device = device\n",
    "            )\n",
    "\n",
    "            self.dataset = {\"TEXT\" : TEXT, \"LABEL\" : LABEL,\n",
    "                    \"train_data\" : train_data, \"valid_data\" : valid_data, \"test_data\" : test_data,\n",
    "                    \"train_iterator\" : train_iterator, \"valid_iterator\" : valid_iterator, \"test_iterator\" : test_iterator}\n",
    "            \n",
    "            pretrained_embeddings = self.dataset[\"TEXT\"].vocab.vectors\n",
    "            print(\"pretrained_embeddings.shape\", pretrained_embeddings.shape)\n",
    "            self.model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            UNK_IDX = self.dataset[\"TEXT\"].vocab.stoi[self.dataset[\"TEXT\"].unk_token]\n",
    "\n",
    "            self.model.embedding.weight.data[UNK_IDX] = torch.zeros(self.model.embedding_dim)\n",
    "            self.model.embedding.weight.data[self.model.pad_idx] = torch.zeros(self.model.embedding_dim)\n",
    "\n",
    "            print(\"self.model.embedding.weight.data\", self.model.embedding.weight.data)\n",
    "            \n",
    "            self.model = LSTM(\n",
    "                vocab_size = len(self.dataset[\"TEXT\"].vocab), \n",
    "                embedding_dim = self.model.embedding_dim, \n",
    "                hidden_dim = self.model.hidden_dim, \n",
    "                output_dim = self.model.output_dim, \n",
    "                n_layers = self.model.n_layers, \n",
    "                bidirectional = self.model.bidirectional, \n",
    "                dropout = self.model.dropout_perent, \n",
    "                pad_idx = self.dataset[\"TEXT\"].vocab.stoi[self.dataset[\"TEXT\"].pad_token] \n",
    "            )\n",
    "\n",
    "            \n",
    "        elif isinstance(self.model, CNN) or isinstance(self.model, CNN1d) :\n",
    "            seed = 1234\n",
    "            max_vocab_size = 25000\n",
    "            batch_size = 64\n",
    "            \n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            \n",
    "            TEXT = data.Field(tokenize = 'spacy', batch_first = True)\n",
    "            LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "            train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "            train_data, valid_data = train_data.split(random_state = random.seed(seed))\n",
    "            print(f'Number of training examples: {len(train_data)}')\n",
    "            print(f'Number of validation examples: {len(valid_data)}')\n",
    "            print(f'Number of testing examples: {len(test_data)}')\n",
    "            \n",
    "            vectors = \"glove.6B.100d\"\n",
    "            unk_init = torch.Tensor.normal_\n",
    "            TEXT.build_vocab(train_data, max_size = max_vocab_size, vectors = vectors, unk_init = unk_init)\n",
    "            LABEL.build_vocab(train_data)\n",
    "            print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "            print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "            \n",
    "            train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "                (train_data, valid_data, test_data), \n",
    "                batch_size = batch_size, device = device\n",
    "            )\n",
    "\n",
    "            self.dataset = {\"TEXT\" : TEXT, \"LABEL\" : LABEL,\n",
    "                            \"train_data\" : train_data, \"valid_data\" : valid_data, \"test_data\" : test_data,\n",
    "                            \"train_iterator\" : train_iterator, \"valid_iterator\" : valid_iterator, \"test_iterator\" : test_iterator}\n",
    "    \n",
    "            if isinstance(self.model, CNN) :\n",
    "                self.model = CNN(\n",
    "                    vocab_size = len(self.dataset[\"TEXT\"].vocab), \n",
    "                    embedding_dim = self.model.embedding_dim, \n",
    "                    n_filters =  self.model.n_filters, \n",
    "                    filter_sizes = self.model.filter_sizes, \n",
    "                    output_dim = self.model.output_dim, \n",
    "                    dropout = self.model.dropout_percent, \n",
    "                    pad_idx = self.dataset[\"TEXT\"].vocab.stoi[self.dataset[\"TEXT\"].pad_token]\n",
    "                )\n",
    "                \n",
    "            else :\n",
    "                self.model = CNN1d(\n",
    "                    vocab_size = len(self.dataset[\"TEXT\"].vocab), \n",
    "                    embedding_dim = self.model.embedding_dim, \n",
    "                    n_filters =  self.model.n_filters, \n",
    "                    filter_sizes = self.model.filter_sizes, \n",
    "                    output_dim = self.model.output_dim, \n",
    "                    dropout = self.model.dropout_percent, \n",
    "                    pad_idx = self.dataset[\"TEXT\"].vocab.stoi[self.dataset[\"TEXT\"].pad_token]\n",
    "                )\n",
    "            \n",
    "        elif isinstance(self.model, BERTGRUSentiment) :\n",
    "            seed = 1234\n",
    "            batch_size = 128\n",
    "            \n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            \n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "            \n",
    "            def tokenize_and_cut(sentence):\n",
    "                tokens = tokenizer.tokenize(sentence) \n",
    "                tokens = tokens[:max_input_length-2]\n",
    "                return tokens\n",
    "                    \n",
    "            init_token_idx = tokenizer.cls_token_id\n",
    "            eos_token_idx = tokenizer.sep_token_id\n",
    "            pad_token_idx = tokenizer.pad_token_id\n",
    "            unk_token_idx = tokenizer.unk_token_id\n",
    "            \n",
    "            TEXT = data.Field(batch_first = True,\n",
    "                              use_vocab = False,\n",
    "                              tokenize = tokenize_and_cut,\n",
    "                              preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                              init_token = init_token_idx,\n",
    "                              eos_token = eos_token_idx,\n",
    "                              pad_token = pad_token_idx,\n",
    "                              unk_token = unk_token_idx)\n",
    "\n",
    "            LABEL = data.LabelField(dtype = torch.float)\n",
    "            \n",
    "            train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "            train_data, valid_data = train_data.split(random_state = random.seed(seed))\n",
    "            print(f\"Number of training examples: {len(train_data)}\")\n",
    "            print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "            print(f\"Number of testing examples: {len(test_data)}\")\n",
    "            \n",
    "            LABEL.build_vocab(train_data)\n",
    "            #print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "            print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "            \n",
    "            train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "                (train_data, valid_data, test_data), \n",
    "                batch_size = batch_size, \n",
    "                device = device)\n",
    "            \n",
    "            self.dataset = {\"TEXT\" : TEXT, \"LABEL\" : LABEL,\n",
    "                            \"train_data\" : train_data, \"valid_data\" : valid_data, \"test_data\" : test_data,\n",
    "                            \"train_iterator\" : train_iterator, \"valid_iterator\" : valid_iterator, \"test_iterator\" : test_iterator}            \n",
    "        \n",
    "        self.optimizer = optimizer if optimizer else optim.Adam(model.parameters(), lr=1e-3)\n",
    "        self.criterion = criterion if criterion else nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.model = self.model.to(device)\n",
    "        self.criterion = self.criterion.to(device)\n",
    "        \n",
    "        self.dump_path = dump_path\n",
    "        \n",
    "    # produces rather large files and generates errors during serialization\n",
    "    \"\"\"\n",
    "    def save_dataset(self, dump_path):\n",
    "        if not os.path.exists(dump_path):\n",
    "            os.makedirs(dump_path)\n",
    "        #pickle.dump(self.dataset, dump_path+'/dataset')\n",
    "        torch.save(self.dataset, dump_path+'/dataset')\n",
    "        self.dump_path = dump_path\n",
    "    \n",
    "    def load_dataset(self, dump_path):\n",
    "        assert os.path.isfile(dump_path+'/dataset'), 'File not found'\n",
    "        #self.dataset = pickle.loard(dump_path+'/dataset')\n",
    "        self.dataset = torch.loard(dump_path+'/dataset')\n",
    "        self.dump_path = dump_path\n",
    "    \"\"\"\n",
    "        \n",
    "    def count_parameters(self):\n",
    "        if not isinstance(self.model, BERTGRUSentiment) :\n",
    "            nb_p = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            print(f'The model has {nb_p:,} trainable parameters')\n",
    "        else :\n",
    "            nb_p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            print(f'The model has {nb_p:,} total trainable parameters')\n",
    "            for name, param in self.model.named_parameters():                \n",
    "                if name.startswith('bert'):\n",
    "                    param.requires_grad = False\n",
    "            nb_p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            print(f'The model has {nb_p:,} trainable parameters')\n",
    "            for name, param in self.model.named_parameters():                \n",
    "                if param.requires_grad:\n",
    "                    print(name)\n",
    "    \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        \"\"\" \n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "        #round predictions to the closest integer\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float() #convert into float for division \n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "    \n",
    "    def train_step(self, iterator):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if isinstance(self.model, LSTM) :\n",
    "                text, text_lengths = batch.text\n",
    "                predictions = self.model(text, text_lengths).squeeze(1)\n",
    "            else :\n",
    "                predictions = self.model(batch.text).squeeze(1)\n",
    "\n",
    "            loss = self.criterion(predictions, batch.label)\n",
    "\n",
    "            acc = self.binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    \n",
    "    def evaluate(self, iterator):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in iterator:\n",
    "                \n",
    "                if isinstance(self.model, LSTM) :\n",
    "                    text, text_lengths = batch.text\n",
    "                    predictions = self.model(text, text_lengths).squeeze(1)\n",
    "                else :\n",
    "                    predictions = self.model(batch.text).squeeze(1)\n",
    "\n",
    "                loss = self.criterion(predictions, batch.label)\n",
    "\n",
    "                acc = self.binary_accuracy(predictions, batch.label)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def epoch_time(self, start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "    \n",
    "    def train(self, n_epochs = 5, dump_id = \"\"):\n",
    "        \n",
    "        assert n_epochs > 0\n",
    "        \n",
    "        dump_id = self.model.getID() if dump_id == \"\" else dump_id\n",
    "              \n",
    "        best_valid_loss = float('inf')\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_loss, train_acc = self.train_step(self.dataset[\"train_iterator\"])\n",
    "            valid_loss, valid_acc = self.evaluate(self.dataset[\"valid_iterator\"])\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n",
    "\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(self.model.state_dict(), self.dump_path+\"/\"+dump_id+'-best-model.pth')\n",
    "\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "                        \n",
    "    \n",
    "    def reload_model(self, dump_id):\n",
    "        dump_id = self.model.getID() if dump_id == \"\" else dump_id\n",
    "        self.model.load_state_dict(torch.load(self.dump_path+\"/\"+dump_id+'-best-model.pth'))\n",
    "    \n",
    "    def test(self): \n",
    "        test_loss, test_acc = self.evaluate(self.dataset[\"test_iterator\"])\n",
    "        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "        \n",
    "    def get_predict_sentiment(self) :\n",
    "        if isinstance(self.model, RNN) or isinstance(self.model, LSTM) :\n",
    "            def predict(sentence):\n",
    "                self.model.eval()\n",
    "                tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "                indexed = [self.dataset[\"TEXT\"].vocab.stoi[t] for t in tokenized]\n",
    "                length = [len(indexed)]\n",
    "                tensor = torch.LongTensor(indexed).to(device)\n",
    "                tensor = tensor.unsqueeze(1)\n",
    "                length_tensor = torch.LongTensor(length)\n",
    "                if isinstance(self.model, RNN) :\n",
    "                    prediction = torch.sigmoid(self.model(tensor)) \n",
    "                else :\n",
    "                    prediction = torch.sigmoid(self.model(tensor, length_tensor)) \n",
    "                return prediction.item()\n",
    "            \n",
    "        elif isinstance(self.model, CNN) or isinstance(self.model, CNN1d):\n",
    "            \n",
    "            def predict(sentence, min_len = 5):\n",
    "                self.model.eval()\n",
    "                tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "                if len(tokenized) < min_len:\n",
    "                    tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "                indexed = [self.dataset[\"TEXT\"].vocab.stoi[t] for t in tokenized]\n",
    "                tensor = torch.LongTensor(indexed).to(device)\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "                prediction = torch.sigmoid(self.model(tensor))\n",
    "                return prediction.item()\n",
    "            \n",
    "        elif isinstance(self.model, BERTGRUSentiment) :\n",
    "            \n",
    "            def predict(tokenizer, sentence):\n",
    "                # bert\n",
    "                self.model.eval()\n",
    "                tokens = tokenizer.tokenize(sentence)\n",
    "                tokens = tokens[:max_input_length-2]\n",
    "                indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "                tensor = torch.LongTensor(indexed).to(device)\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "                prediction = torch.sigmoid(model(tensor))\n",
    "                return prediction.item()\n",
    "        else :\n",
    "            def predict(sentence):\n",
    "                return\n",
    "        \n",
    "        return predict\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['How', 'can', 'a', 'movie', 'with', 'Amy', ',', 'Posey', 'and', 'Raechel', 'have', 'NOTHING', 'funny', 'in', 'it', '?', 'Believe', 'it', 'or', 'not', \"'\", 'House', 'Bunny', \"'\", 'did', 'this', 'better', 'and', 'funnier', '.', 'Hopefully', 'the', 'principals', 'had', 'a', 'good', 'holiday', 'and', 'got', 'some', 'money', '-', 'this', 'movie', 'is', 'an', 'embarrassment', 'to', 'all', 'of', 'them', '.', 'It', 'is', 'a', 'cliché', 'from', 'beginning', 'to', 'end', '.', 'Clichés', 'can', 'work', 'well', 'with', 'a', 'script', ',', 'or', 'at', 'least', 'an', 'idea', '.', 'This', 'movie', 'does', 'nothing', 'but', 'use', 'cliché', 'after', 'cliché', 'rather', 'than', 'ideas', 'or', 'script', '.', 'It', 'uses', 'the', 'preexisting', 'persona', \"'s\", 'of', 'the', 'actresses', 'rather', 'than', 'develop', 'characters', '.', 'Bad', ',', 'sad', ',', 'and', 'rubbish', '.', 'Now', 'I', 'apparently', 'have', 'to', 'have', 'ten', 'lines', 'of', 'text', 'for', 'a', 'comment', '.', 'Really', '?', 'Why', '?', 'As', 'an', 'IT', 'ops', 'manager', 'this', 'is', 'another', 'example', 'of', 'sloppy', 'coding', '.'], 'label': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(dataset[\"train_data\"].examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 202820), (',', 192918), ('.', 164754), ('and', 109435), ('a', 109381), ('of', 101102), ('to', 93825), ('is', 76552), ('in', 61403), ('I', 54228), ('it', 53519), ('that', 49356), ('\"', 44224), (\"'s\", 43374), ('this', 42438), ('-', 37284), ('/><br', 35417), ('was', 34920), ('as', 30245), ('with', 29869)]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"TEXT\"].vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"TEXT\"].vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"LABEL\"].vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(dataset[\"TEXT\"].vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n",
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n",
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, criterion, dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.700 | Train Acc: 50.19%\n",
      "\t Val. Loss: 0.868 |  Val. Acc: 49.88%\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.900 | Test Acc: 48.30%\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() #.predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8343528509140015\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"What can you do with the fire ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5052047967910767\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"NLP is awesome !\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8909756541252136\n",
      "0.19438283145427704\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"Corona-virus has killed the whole world, the economy is suffering, empires have fallen, what a disaster !\"))\n",
    "print(predict(\"Gone has killed the whole world, the economy is suffering, empires have fallen, what a disaster !\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 25002 # will be update to len(dataset[\"TEXT\"].vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = 1 # will be update to dataset[\"TEXT\"].vocab.stoi[dataset[\"TEXT\"].pad_token]\n",
    "\n",
    "model = LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n",
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n",
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "pretrained_embeddings.shape torch.Size([25002, 100])\n",
      "self.model.embedding.weight.data tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.2295, -0.4421,  0.4331,  ..., -0.3084, -0.3175,  0.3562],\n",
      "        [-0.0046, -0.4325, -0.3023,  ...,  0.3291, -0.1993, -0.2312],\n",
      "        [-0.0212,  0.5229,  0.7983,  ..., -0.4114,  0.0744,  0.6785]])\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, criterion, dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 37s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.68%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 49.74%\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM =   100 # will be update to len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = 1 # will be update to TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 130,601 trainable parameters\n",
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n",
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, criterion, dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.829 | Train Acc: 50.64%\n",
      "\t Val. Loss: 0.754 |  Val. Acc: 49.26%\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.748 | Test Acc: 49.99%\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() #.predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(\"What can you do with the fire ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "model = BERTGRUSentiment(bert, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 112,241,409 total trainable parameters\n",
      "The model has 2,759,169 trainable parameters\n",
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n",
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, criterion, dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(n_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() #.predict_sentiment()\n",
    "print(predict(\"What can you do with the fire ?\"))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
