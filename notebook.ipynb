{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import RNN, LSTM, CNN, CNN1d, BERTGRUSentiment, Trainer\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main concepts of TorchText is the Field. These define how your data should be processed. In our sentiment classification task the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
    "The parameters of a Field specify how the data should be processed. We use the TEXT field to define how the review should be processed, and the LABEL field to process the sentiment.\n",
    "Our TEXT field has tokenize='spacy' as an argument. This defines that the \"tokenization\" (the act of splitting the string into\n",
    "discrete \"tokens\") should be done using the spaCy tokenizer. If no tokenize argument is passed, the default is simply splitting the string on spaces.\n",
    "LABEL is defined by a LabelField, a special subset of the Field class specifically used for handling labels. We will explain the dtype argument later.\n",
    "Another handy feature of TorchText is that it has support for common datasets used in natural language processing (NLP) such as IMDb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will proceed as follows:\n",
    "- We'll define our model and pass it on to our trainer...\n",
    "- Then we will compile our trainer, which will load the data, build the optimizer and the loss function, and update the model parameters if necessary.\n",
    "- After that we will review some specificities (model, data...) of our trainer when necessary.\n",
    "- We'll continue by training and testing our model. \n",
    "- And finally, we'll test our model on a few real-life cases.\n",
    "\n",
    "See the src/model.py script for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Details\n",
    "\n",
    "our `RNN` class is a sub-class of `nn.Module` and the use of `super`.\n",
    "\n",
    "Within the `__init__` we define the _layers_ of the module. Our three layers are an _embedding_ layer, our RNN, and a _linear_ layer. All layers have their parameters initialized to random values, unless explicitly specified.\n",
    "\n",
    "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see [here](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/).\n",
    "\n",
    "The RNN layer is our RNN which takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, h_t.\n",
    "\n",
    "![](assets/sentiment7.png)\n",
    "\n",
    "Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension.\n",
    "\n",
    "The `forward` method is called when we feed examples into our model.\n",
    "\n",
    "Each batch, `text`, is a tensor of size _**[sentence length, batch size]**_. That is a batch of sentences, each having each word converted into a one-hot vector. \n",
    "\n",
    "You may notice that this tensor should have another dimension due to the one-hot vectors, however PyTorch conveniently stores a one-hot vector as it's index value, i.e. the tensor representing a sentence is just a tensor of the indexes for each token in that sentence. The act of converting a list of tokens into a list of indexes is commonly called *numericalizing*.\n",
    "\n",
    "The input batch is then passed through the embedding layer to get `embedded`, which gives us a dense vector representation of our sentences. `embedded` is a tensor of size _**[sentence length, batch size, embedding dim]**_.\n",
    "\n",
    "`embedded` is then fed into the RNN. In some frameworks you must feed the initial hidden state, $h_0$, into the RNN, however in PyTorch, if no initial hidden state is passed as an argument it defaults to a tensor of all zeros.\n",
    "\n",
    "The RNN returns 2 tensors, `output` of size _**[sentence length, batch size, hidden dim]**_ and `hidden` of size _**[1, batch size, hidden dim]**_. `output` is the concatenation of the hidden state from every time step, whereas `hidden` is simply the final hidden state. We verify this using the `assert` statement. Note the `squeeze` method, which is used to remove a dimension of size 1. \n",
    "\n",
    "Finally, we feed the last hidden state, `hidden`, through the linear layer, `fc`, to produce a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dimension is the dimension of the one-hot vectors, which is equal to the vocabulary size. \n",
    "\n",
    "The embedding dimension is the size of the dense word vectors. This is usually around 50-250 dimensions, but depends on the size of the vocabulary.\n",
    "\n",
    "The hidden dimension is the size of the hidden states. This is usually around 100-500 dimensions, but also depends on factors such as on the vocabulary size, the size of the dense vectors and the complexity of the task.\n",
    "\n",
    "The output dimension is usually the number of classes, however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, i.e. a single scalar real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = RNN(\n",
    "        input_dim = 1, # will be update to len(dataset[\"TEXT\"].vocab) during compilation\n",
    "        embedding_dim = 100, \n",
    "        hidden_dim = 256, \n",
    "        output_dim = 1 \n",
    "    ), \n",
    "    dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll create an optimizer. This is the algorithm we use to update the parameters of the module. Here, we'll use _stochastic gradient descent_ (SGD). \n",
    "\n",
    "The loss function here is binary cross entropy with logits. Our model currently outputs an unbound real number. As our labels are either 0 or 1, we want to restrict the predictions to a number between 0 and 1. We do this using the sigmoid or logit functions.\n",
    "We then use this this bound scalar to calculate the loss using binary cross entropy.\n",
    "The BCEWithLogitsLoss criterion carries out both the sigmoid and the binary cross entropy steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 20000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 25000\n",
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "The model has 2,592,105 trainable parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.compile(\n",
    "    optimizer = \"SGD\", # algorithm we use to update the parameters of the module\n",
    "    criterion = \"BCEWithLogitsLoss\", # loss function\n",
    "    seed = 1234, # random seeds for reproducibility\n",
    "    split_ratio = 0.8, # ratio of training data to use for training, the rest for validation\n",
    "    batch_size = 64, # batch size\n",
    "    max_vocab_size = 25000 # maximun token in the vocabulary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['How', 'can', 'a', 'movie', 'with', 'Amy', ',', 'Posey', 'and', 'Raechel', 'have', 'NOTHING', 'funny', 'in', 'it', '?', 'Believe', 'it', 'or', 'not', \"'\", 'House', 'Bunny', \"'\", 'did', 'this', 'better', 'and', 'funnier', '.', 'Hopefully', 'the', 'principals', 'had', 'a', 'good', 'holiday', 'and', 'got', 'some', 'money', '-', 'this', 'movie', 'is', 'an', 'embarrassment', 'to', 'all', 'of', 'them', '.', 'It', 'is', 'a', 'cliché', 'from', 'beginning', 'to', 'end', '.', 'Clichés', 'can', 'work', 'well', 'with', 'a', 'script', ',', 'or', 'at', 'least', 'an', 'idea', '.', 'This', 'movie', 'does', 'nothing', 'but', 'use', 'cliché', 'after', 'cliché', 'rather', 'than', 'ideas', 'or', 'script', '.', 'It', 'uses', 'the', 'preexisting', 'persona', \"'s\", 'of', 'the', 'actresses', 'rather', 'than', 'develop', 'characters', '.', 'Bad', ',', 'sad', ',', 'and', 'rubbish', '.', 'Now', 'I', 'apparently', 'have', 'to', 'have', 'ten', 'lines', 'of', 'text', 'for', 'a', 'comment', '.', 'Really', '?', 'Why', '?', 'As', 'an', 'IT', 'ops', 'manager', 'this', 'is', 'another', 'example', 'of', 'sloppy', 'coding', '.'], 'label': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(trainer.dataset[\"train_data\"].examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the vocab size 25002 and not 25000? One of the addition tokens is the `<unk>` token and the other is a `<pad>` token.\n",
    "\n",
    "When we feed sentences into our model, we feed a _batch_ of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size. Thus, to ensure each sentence in the batch is the same size, any shorter than the longest within the batch are padded.\n",
    "\n",
    "We can also view the most common words in the vocabulary and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 231788), (',', 220817), ('.', 188731), ('and', 125386), ('a', 125106), ('of', 115329), ('to', 107262), ('is', 87345), ('in', 70171), ('I', 62113), ('it', 61318), ('that', 56414), ('\"', 50855), (\"'s\", 49674), ('this', 48463), ('-', 42447), ('/><br', 40540), ('was', 40037), ('as', 34679), ('with', 34232)]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.dataset[\"TEXT\"].vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the vocabulary directly using either the `stoi` (string to int) or `itos` (int to string) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(trainer.dataset[\"TEXT\"].vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the labels, ensuring 0 is for negative and 1 is for positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(trainer.dataset[\"LABEL\"].vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "The `train_step` function iterates over all examples, one batch at a time. \n",
    "\n",
    "`model.train()` is used to put the model in \"training mode\", which turns on _dropout_ and _batch normalization_. Although we aren't using them in this model, it's good practice to include it.\n",
    "\n",
    "For each batch, we first zero the gradients. Each parameter in a model has a `grad` attribute which stores the gradient calculated by the `criterion`. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
    "\n",
    "We then feed the batch of sentences, `batch.text`, into the model. Note, you do not need to do `model.forward(batch.text)`, simply calling the model works. The `squeeze` is needed as the predictions are initially size _**[batch size, 1]**_, and we need to remove the dimension of size 1 as PyTorch expects the predictions input to our criterion function to be of size _**[batch size]**_.\n",
    "\n",
    "The loss and accuracy are then calculated using our predictions and the labels, `batch.label`, with the loss being averaged over all examples in the batch.\n",
    "\n",
    "We calculate the gradient of each parameter with `loss.backward()`, and then update the parameters using the gradients and optimizer algorithm with `optimizer.step()`.\n",
    "\n",
    "The loss and accuracy is accumulated across the epoch, the `.item()` method is used to extract a scalar from a tensor which only contains a single value.\n",
    "\n",
    "Finally, we return the loss and accuracy, averaged across the epoch. The `len` of an iterator is the number of batches in the iterator.\n",
    "\n",
    "You may recall when initializing the `LABEL` field, we set `dtype=torch.float`. This is because TorchText sets tensors to be `LongTensor`s by default, however our criterion expects both inputs to be `FloatTensor`s. Setting the `dtype` to be `torch.float`, did this for us. The alternative method of doing this would be to do the conversion inside the `train` function by passing `batch.label.float()` instad of `batch.label` to the criterion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`evaluate` is similar to `train_step`, with a few modifications as you don't want to update the parameters when evaluating.\n",
    "\n",
    "`model.eval()` puts the model in \"evaluation mode\", this turns off _dropout_ and _batch normalization_. Again, we are not using them in this model, but it is good practice to include them.\n",
    "\n",
    "No gradients are calculated on PyTorch operations inside the `with no_grad()` block. This causes less memory to be used and speeds up computation.\n",
    "\n",
    "The rest of the function is the same as `train`, with the removal of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as we do not update the model's parameters when evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 38s\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.87%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.11%\n",
      "\tNew best validation score\n",
      "Epoch: 02 | Epoch Time: 0m 38s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.34%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 50.63%\n",
      "\tNot a better validation score (0 / 2).\n",
      "Epoch: 03 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.33%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.33%\n",
      "\tNew best validation score\n",
      "Epoch: 04 | Epoch Time: 0m 28s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.55%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.11%\n",
      "\tNew best validation score\n",
      "Epoch: 05 | Epoch Time: 0m 38s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.01%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.23%\n",
      "\tNew best validation score\n",
      "Epoch: 06 | Epoch Time: 0m 38s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.01%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 50.65%\n",
      "\tNot a better validation score (0 / 2).\n",
      "Epoch: 07 | Epoch Time: 0m 39s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.71%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.56%\n",
      "\tNot a better validation score (1 / 2).\n",
      "Epoch: 08 | Epoch Time: 0m 38s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.01%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 50.65%\n",
      "\tNot a better validation score (2 / 2).\n"
     ]
    }
   ],
   "source": [
    "stats = trainer.train(\n",
    "    max_epochs = 50, # maximun number of epochs\n",
    "    improving_limit = 2, # If the precision of the model does not improve during `improving_limit` epoch, we stop training and keep the best model.\n",
    "    dump_id = \"\" # identifier to distinguish models in the serialization folder, is by default equal to the name of the base model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (8,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8cafb86a8d25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/wl_research/wl_research_challenge/src/model.py\u001b[0m in \u001b[0;36mplot_statistics\u001b[0;34m(self, statistics)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \"\"\"\n\u001b[1;32m   1645\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (8,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASQklEQVR4nO3dX4id933n8fdnxxZstCZpY8UNktJqi7ZeF2JwzsrpJpvYuzgrmQYRyIW8IQETEO5GpfSiVOyFc7E3u+RmSVeJEEGEXMS62MTJLNiWA2XXoa53daY4tuRWYVZJq0EBj2zj0KRUKP3uxXnEnE5mNI9mjuaM5/d+wWHO8/tz5vf8mDmfeZ45z/NLVSFJatc/mfYAJEnTZRBIUuMMAklqnEEgSY0zCCSpcQaBJDVuzSBIcjrJ60nOr1KfJF9OMp/klSQPjNUdTHKxqzs+yYFLkiajzxHB14GDN6k/BOzvHkeBrwIkmQFOdPX3AY8luW8jg5UkTd6aQVBVLwBv3qTJYeAbNfIS8J4k7wcOAPNVdamqrgFnuraSpC3kjgm8xm7g8tj2Qle2UvmDq71IkqOMjijYuXPnh+69994JDE2S2jA3N3e1qnatp+8kgiArlNVNyldUVaeAUwCDwaCGw+EEhiZJbUjy1+vtO4kgWAD2jm3vAa4AO1YplyRtIZP4+Ogs8Lnu00MfBt6uqp8A54D9SfYl2QEc6dpKkraQNY8IkjwFPATcnWQB+CJwJ0BVnQSeAR4F5oGfA493ddeTHAPOAjPA6aq6cBv2QZK0AWsGQVU9tkZ9AV9Ype4ZRkEhSdqivLJYkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktS4XkGQ5GCSi0nmkxxfof6PkrzcPc4n+UWSX+3qfpzk1a7OFeklaYvps1TlDHACeITRQvXnksxW1Ws32lTVl4Avde0/CfxhVb059jIPV9XViY5ckjQRfY4IDgDzVXWpqq4BZ4DDN2n/GPDUJAYnSbr9+gTBbuDy2PZCV/ZLkrwLOAh8a6y4gOeTzCU5uto3SXI0yTDJcHFxscewJEmT0CcIskJZrdL2k8CfLTst9JGqegA4BHwhycdW6lhVp6pqUFWDXbt29RiWJGkS+gTBArB3bHsPcGWVtkdYdlqoqq50X18HnmZ0qkmStEX0CYJzwP4k+5LsYPRmP7u8UZJ3Ax8HvjtWtjPJXTeeA58Azk9i4JKkyVjzU0NVdT3JMeAsMAOcrqoLSZ7o6k92TT8FPF9VPxvrfg/wdJIb3+ubVfXcJHdAkrQxqVrtdP/0DAaDGg695ECS+koyV1WD9fT1ymJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuN6BUGSg0kuJplPcnyF+oeSvJ3k5e7xZN++kqTpWnOpyiQzwAngEUYL2Z9LMltVry1r+v2q+t119pUkTUmfI4IDwHxVXaqqa8AZ4HDP199IX0nSJugTBLuBy2PbC13Zcr+T5AdJnk3y27fYlyRHkwyTDBcXF3sMS5I0CX2CICuULV/x/i+AX6+q+4E/Ab5zC31HhVWnqmpQVYNdu3b1GJYkaRL6BMECsHdsew9wZbxBVf20qv62e/4McGeSu/v0lSRNV58gOAfsT7IvyQ7gCDA73iDJryVJ9/xA97pv9OkrSZquNT81VFXXkxwDzgIzwOmqupDkia7+JPBp4PeSXAf+DjhSVQWs2Pc27YskaR0yer/eWgaDQQ2Hw2kPQ5LeMZLMVdVgPX29sliSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTG9QqCJAeTXEwyn+T4CvWfSfJK93gxyf1jdT9O8mqSl5O4yIAkbTFrrlCWZAY4ATzCaA3ic0lmq+q1sWY/Aj5eVW8lOQScAh4cq3+4qq5OcNySpAnpc0RwAJivqktVdQ04Axweb1BVL1bVW93mS4wWqZckvQP0CYLdwOWx7YWubDWfB54d2y7g+SRzSY6u1inJ0STDJMPFxcUew5IkTcKap4aArFC24kLHSR5mFAQfHSv+SFVdSfI+4HtJ/qqqXvilF6w6xeiUEoPBYOstpCxJ21SfI4IFYO/Y9h7gyvJGST4IfA04XFVv3Civqivd19eBpxmdapIkbRF9guAcsD/JviQ7gCPA7HiDJB8Avg18tqp+OFa+M8ldN54DnwDOT2rwkqSNW/PUUFVdT3IMOAvMAKer6kKSJ7r6k8CTwHuBryQBuF5VA+Ae4Omu7A7gm1X13G3ZE0nSuqRq652OHwwGNRx6yYEk9ZVkrvsD/JZ5ZbEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXG9giDJwSQXk8wnOb5CfZJ8uat/JckDfftKkqZrzSBIMgOcAA4B9wGPJblvWbNDwP7ucRT46i30lSRNUZ8jggPAfFVdqqprwBng8LI2h4Fv1MhLwHuSvL9nX0nSFK25eD2wG7g8tr0APNijze6efQFIcpTR0QTA3yc532NsLbgbuDrtQWwBzsMS52KJc7Hkt9bbsU8QZIWy5Sver9amT99RYdUp4BRAkuF6F2HebpyLEedhiXOxxLlYkmS43r59gmAB2Du2vQe40rPNjh59JUlT1Od/BOeA/Un2JdkBHAFml7WZBT7XfXrow8DbVfWTnn0lSVO05hFBVV1Pcgw4C8wAp6vqQpInuvqTwDPAo8A88HPg8Zv17TGuU+vZmW3KuRhxHpY4F0uciyXrnotUrXjKXpLUCK8slqTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGtdnqcrTSV5fbaEY1yuWpHe2PkcEXwcO3qTe9Yol6R1szSCoqheAN2/SxPWKJekdrM8KZWvZ8HrF8I/XLN65c+eH7r333gkMTZLaMDc3d7Wqdq2n7ySCYMPrFcM/XrN4MBjUcLju5TclqTlJ/nq9fScRBK5XLEnvYJP4+KjrFUvSO9iaRwRJngIeAu5OsgB8EbgTbtt6xZKkTdRn8frH1qgv4Aur1D3DKCgkSVuUVxZLUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhrXKwiSHExyMcl8kuMr1P9Rkpe7x/kkv0jyq13dj5O82tW5Ir0kbTF9lqqcAU4AjzBaqP5cktmqeu1Gm6r6EvClrv0ngT+sqjfHXubhqro60ZFLkiaizxHBAWC+qi5V1TXgDHD4Ju0fA56axOAkSbdfnyDYDVwe217oyn5JkncBB4FvjRUX8HySuSRHV/smSY4mGSYZLi4u9hiWJGkS+gRBViirVdp+EvizZaeFPlJVDwCHgC8k+dhKHavqVFUNqmqwa9euHsOSJE1CnyBYAPaObe8BrqzS9gjLTgtV1ZXu6+vA04xONUmStog+QXAO2J9kX5IdjN7sZ5c3SvJu4OPAd8fKdia568Zz4BPA+UkMXJI0GWt+aqiqric5BpwFZoDTVXUhyRNd/cmu6aeA56vqZ2Pd7wGeTnLje32zqp6b5A5IkjYmVaud7p+ewWBQw6GXHEhSX0nmqmqwnr5eWSxJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjegVBkoNJLiaZT3J8hfqHkryd5OXu8WTfvpKk6VpzhbIkM8AJ4BFG6xefSzJbVa8ta/r9qvrddfaVJE1JnyOCA8B8VV2qqmvAGeBwz9ffSF9J0iboEwS7gctj2wtd2XK/k+QHSZ5N8tu32JckR5MMkwwXFxd7DEuSNAl9giArlC1f6PgvgF+vqvuBPwG+cwt9R4VVp6pqUFWDXbt29RiWJGkS+gTBArB3bHsPcGW8QVX9tKr+tnv+DHBnkrv79JUkTVefIDgH7E+yL8kO4AgwO94gya8lSff8QPe6b/TpK0marjU/NVRV15McA84CM8DpqrqQ5Imu/iTwaeD3klwH/g44UlUFrNj3Nu2LJGkdMnq/3loGg0ENh8NpD0OS3jGSzFXVYD19vbJYkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktS4XkGQ5GCSi0nmkxxfof4zSV7pHi8muX+s7sdJXk3ychJXm5GkLWbNpSqTzAAngEcYLUZ/LslsVb021uxHwMer6q0kh4BTwINj9Q9X1dUJjluSNCF9jggOAPNVdamqrgFngMPjDarqxap6q9t8Cdgz2WFKkm6XPkGwG7g8tr3Qla3m88CzY9sFPJ9kLsnR1TolOZpkmGS4uLjYY1iSpElY89QQkBXKVlzxPsnDjILgo2PFH6mqK0neB3wvyV9V1Qu/9IJVpxidUmIwGKz4+pKkyetzRLAA7B3b3gNcWd4oyQeBrwGHq+qNG+VVdaX7+jrwNKNTTZKkLaJPEJwD9ifZl2QHcASYHW+Q5APAt4HPVtUPx8p3JrnrxnPgE8D5SQ1ekrRxa54aqqrrSY4BZ4EZ4HRVXUjyRFd/EngSeC/wlSQA16tqANwDPN2V3QF8s6qeuy17Iklal1RtvdPxg8GghkMvOZCkvpLMdX+A3zKvLJakxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNa5XECQ5mORikvkkx1eoT5Ivd/WvJHmgb19J0nStGQRJZoATwCHgPuCxJPcta3YI2N89jgJfvYW+kqQp6nNEcACYr6pLVXUNOAMcXtbmMPCNGnkJeE+S9/fsK0maojUXrwd2A5fHtheAB3u02d2zLwBJjjI6mgD4+yTne4ytBXcDV6c9iC3AeVjiXCxxLpb81no79gmCrFC2fMX71dr06TsqrDoFnAJIMlzvIszbjXMx4jwscS6WOBdLkgzX27dPECwAe8e29wBXerbZ0aOvJGmK+vyP4BywP8m+JDuAI8DssjazwOe6Tw99GHi7qn7Ss68kaYrWPCKoqutJjgFngRngdFVdSPJEV38SeAZ4FJgHfg48frO+PcZ1aj07s005FyPOwxLnYolzsWTdc5GqFU/ZS5Ia4ZXFktQ4g0CSGje1INjIbSu2mx5z8ZluDl5J8mKS+6cxzs3Q95YkSf5Vkl8k+fRmjm8z9ZmLJA8leTnJhST/e7PHuFl6/I68O8n/TPKDbi4en8Y4b7ckp5O8vtp1Vut+36yqTX8w+sfx/wP+OaOPmP4AuG9Zm0eBZxldi/Bh4P9MY6xbZC7+NfAr3fNDLc/FWLs/ZfQhhU9Pe9xT/Ll4D/Aa8IFu+33THvcU5+I/Af+1e74LeBPYMe2x34a5+BjwAHB+lfp1vW9O64hgI7et2G7WnIuqerGq3uo2X2J0PcZ21PeWJL8PfAt4fTMHt8n6zMV/AL5dVX8DUFXbdT76zEUBdyUJ8M8YBcH1zR3m7VdVLzDat9Ws631zWkGw2i0pbrXNdnCr+/l5Rom/Ha05F0l2A58CTm7iuKahz8/FvwB+Jcn/SjKX5HObNrrN1Wcu/jvwLxldsPoq8AdV9Q+bM7wtZV3vm32uLL4dNnLbiu2m934meZhREHz0to5oevrMxX8D/riqfjH642/b6jMXdwAfAv4d8E+BP0/yUlX98HYPbpP1mYt/D7wM/FvgN4HvJfl+Vf30dg9ui1nX++a0gmAjt63YbnrtZ5IPAl8DDlXVG5s0ts3WZy4GwJkuBO4GHk1yvaq+szlD3DR9f0euVtXPgJ8leQG4H9huQdBnLh4H/kuNTpTPJ/kRcC/wfzdniFvGut43p3VqaCO3rdhu1pyLJB8Avg18dhv+tTduzbmoqn1V9RtV9RvA/wD+4zYMAej3O/Jd4N8kuSPJuxjd2fcvN3mcm6HPXPwNoyMjktzD6E6clzZ1lFvDut43p3JEUBu4bcV203MungTeC3yl+0v4em3DOy72nIsm9JmLqvrLJM8BrwD/AHytqrbd7dt7/lz8Z+DrSV5ldHrkj6tq292eOslTwEPA3UkWgC8Cd8LG3je9xYQkNc4riyWpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJatz/B2fhMT8jTbHmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot_statistics(statistics = stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.709 | Test Acc: 47.32%\n"
     ]
    }
   ],
   "source": [
    "trainer.test(dump_id = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() #.predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5184460282325745\n",
      "0.5323715209960938\n"
     ]
    }
   ],
   "source": [
    "# example negative review...\n",
    "print(predict(sentence = \"This film is too scary, too much gunfire and blood spilled inside. I can't watch bad movies like this anymore.\"))\n",
    "# example positive review...\n",
    "print(predict(sentence = \"Among these actors, I prefer the most romantic one, he likes what he does, is positive about chess and knows how to celebrate victories.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Details\n",
    "\n",
    "Another addition to this model is that we are not going to learn the embedding for the `<pad>` token. This is because we want to explitictly tell our model that padding tokens are irrelevant to determining the sentiment of a sentence. This means the embedding for the pad token will remain at what it is initialized to (we initialize it to all zeros later). We do this by passing the index of our pad token as the `padding_idx` argument to the `nn.Embedding` layer.\n",
    "\n",
    "To use an LSTM instead of the standard RNN, we use `nn.LSTM` instead of `nn.RNN`. Also, note that the LSTM returns the `output` and a tuple of the final `hidden` state and the final `cell` state, whereas the standard RNN only returned the `output` and final `hidden` state. \n",
    "\n",
    "As the final hidden state of our LSTM has both a forward and a backward component, which will be concatenated together, the size of the input to the `nn.Linear` layer is twice that of the hidden dimension size.\n",
    "\n",
    "Implementing bidirectionality and adding additional layers are done by passing values for the `num_layers` and `bidirectional` arguments for the RNN/LSTM. \n",
    "\n",
    "Dropout is implemented by initializing an `nn.Dropout` layer (the argument is the probability of dropping out each neuron) and using it within the `forward` method after each layer we want to apply dropout to. **Note**: never use dropout on the input or output layers (`text` or `fc` in this case), you only ever want to use dropout on intermediate layers. The LSTM has a `dropout` argument which adds dropout on the connections between hidden states in one layer to hidden states in the next layer. \n",
    "\n",
    "As we are passing the lengths of our sentences to be able to use packed padded sequences, we have to add a second argument, `text_lengths`, to `forward`. \n",
    "\n",
    "Before we pass our embeddings to the RNN, we need to pack them, which we do with `nn.utils.rnn.packed_padded_sequence`. This will cause our RNN to only process the non-padded elements of our sequence. The RNN will then return `packed_output` (a packed sequence) as well as the `hidden` and `cell` states (both of which are tensors). Without packed padded sequences, `hidden` and `cell` are tensors from the last element in the sequence, which will most probably be a pad token, however when using packed padded sequences they are both from the last non-padded element in the sequence. \n",
    "\n",
    "We then unpack the output sequence, with `nn.utils.rnn.pad_packed_sequence`, to transform it from a packed sequence to a tensor. The elements of `output` from padding tokens will be zero tensors (tensors where every element is zero). Usually, we only have to unpack output if we are going to use it later on in the model. Although we aren't in this case, we still unpack the sequence just to show how it is done.\n",
    "\n",
    "The final hidden state, `hidden`, has a shape of _**[num layers * num directions, batch size, hid dim]**_. These are ordered: **[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]**. As we want the final (top) layer forward and backward hidden states, we get the top two hidden layers from the first dimension, `hidden[-2,:,:]` and `hidden[-1,:,:]`, and concatenate them together before passing them to the linear layer (after applying dropout). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we'll create an instance of our LSTM class, with the new parameters and arguments for the number of layers, bidirectionality and dropout probability.\n",
    "To ensure the pre-trained vectors can be loaded into the model, the `embedding_dim` must be equal to that of the pre-trained GloVe vectors loaded earlier.\n",
    "\n",
    "We get our pad token index from the vocabulary, getting the actual string representing the pad token from the field's `pad_token` attribute, which is `<pad>` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = LSTM(\n",
    "        vocab_size = 25002, # will be update to len(dataset[\"TEXT\"].vocab) during compilation\n",
    "        embedding_dim = 100, \n",
    "        hidden_dim = 256, \n",
    "        output_dim = 1, \n",
    "        n_layers = 2, \n",
    "        bidirectional = True, \n",
    "        dropout = 0.5, \n",
    "        pad_idx = 1 # will be update to dataset[\"TEXT\"].vocab.stoi[dataset[\"TEXT\"].pad_token] during compilation\n",
    "    ), \n",
    "    dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to training the model.\n",
    "\n",
    "The only change we'll make here is changing the optimizer from `SGD` to `Adam`. SGD updates all parameters with the same learning rate and choosing this learning rate can be tricky. `Adam` adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates. More information about `Adam` (and other optimizers) can be found [here](http://ruder.io/optimizing-gradient-descent/index.html).\n",
    "\n",
    "To change `SGD` to `Adam`, we simply change `optim.SGD` to `optim.Adam`, also note how we do not have to provide an initial learning rate for Adam as PyTorch specifies a sensibile default initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.compile(\n",
    "    optimizer = \"Adam\", # algorithm we use to update the parameters of the module\n",
    "    criterion = \"BCEWithLogitsLoss\", # loss function\n",
    "    seed = 1234, # random seeds for reproducibility\n",
    "    split_ratio = 0.8, # ratio of training data to use for training, the rest for validation\n",
    "    batch_size = 4, # batch size\n",
    "    max_vocab_size = 25000 # maximun token in the vocabulary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the first two rows of the embedding weights matrix have been set to zeros. As we passed the index of the pad token to the `padding_idx` of the embedding layer it will remain zeros throughout training, however the `<unk>` token embedding will be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    max_epochs = 50, # maximun number of epochs\n",
    "    improving_limit = 2, # If the precision of the model does not improve during `improving_limit` epoch, we stop training and keep the best model.\n",
    "    dump_id = \"\" # identifier to distinguish models in the serialization folder, is by default equal to the name of the base model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and get our new and vastly improved test accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(dump_id=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "\n",
    "We can now use our model to predict the sentiment of any sentence we give it. As it has been trained on movie reviews, the sentences provided should also be movie reviews.\n",
    "\n",
    "When using a model for inference it should always be in evaluation mode. If this tutorial is followed step-by-step then it should already be in evaluation mode (from doing `evaluate` on the test set), however we explicitly set it to avoid any risk.\n",
    "\n",
    "Our `predict_sentiment` function does a few things:\n",
    "- sets the model to evaluation mode\n",
    "- tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n",
    "- indexes the tokens by converting them into their integer representation from our vocabulary\n",
    "- gets the length of our sequence\n",
    "- converts the indexes, which are a Python list into a PyTorch tensor\n",
    "- add a batch dimension by `unsqueeze`ing \n",
    "- converts the length into a tensor\n",
    "- squashes the output prediction from a real number between 0 and 1 with the `sigmoid` function\n",
    "- converts the tensor holding a single value into an integer with the `item()` method\n",
    "\n",
    "We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() #.predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example negative review...\n",
    "print(predict(sentence = \"What can you do with the fire ?\"))\n",
    "# example positive review...\n",
    "print(predict(sentence = \"What can you do with the fire ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model \n",
    "\n",
    "Now to build our model.\n",
    "\n",
    "The first major hurdle is visualizing how CNNs are used for text. Images are typically 2 dimensional (we'll ignore the fact that there is a third \"colour\" dimension for now) whereas text is 1 dimensional. However, we know that the first step in almost all of our previous tutorials (and pretty much all NLP pipelines) is converting the words into word embeddings. This is how we can visualize our words in 2 dimensions, each word along one axis and the elements of vectors aross the other dimension. Consider the 2 dimensional representation of the embedded sentence below:\n",
    "\n",
    "![](assets/sentiment9.png)\n",
    "\n",
    "We can then use a filter that is **[n x emb_dim]**. This will cover $n$ sequential words entirely, as their width will be `emb_dim` dimensions. Consider the image below, with our word vectors are represented in green. Here we have 4 words with 5 dimensional embeddings, creating a [4x5] \"image\" tensor. A filter that covers two words at a time (i.e. bi-grams) will be **[2x5]** filter, shown in yellow, and each element of the filter with have a _weight_ associated with it. The output of this filter (shown in red) will be a single real number that is the weighted sum of all elements covered by the filter.\n",
    "\n",
    "![](assets/sentiment12.png)\n",
    "\n",
    "The filter then moves \"down\" the image (or across the sentence) to cover the next bi-gram and another output (weighted sum) is calculated. \n",
    "\n",
    "![](assets/sentiment13.png)\n",
    "\n",
    "Finally, the filter moves down again and the final output for this filter is calculated.\n",
    "\n",
    "![](assets/sentiment14.png)\n",
    "\n",
    "In our case (and in the general case where the width of the filter equals the width of the \"image\"), our output will be a vector with number of elements equal to the height of the image (or lenth of the word) minus the height of the filter plus one, $4-2+1=3$ in this case.\n",
    "\n",
    "This example showed how to calculate the output of one filter. Our model (and pretty much all CNNs) will have lots of these filters. The idea is that each filter will learn a different feature to extract. In the above example, we are hoping each of the **[2 x emb_dim]** filters will be looking for the occurence of different bi-grams. \n",
    "\n",
    "In our model, we will also have different sizes of filters, heights of 3, 4 and 5, with 100 of each of them. The intuition is that we will be looking for the occurence of different tri-grams, 4-grams and 5-grams that are relevant for analysing sentiment of movie reviews.\n",
    "\n",
    "The next step in our model is to use *pooling* (specifically *max pooling*) on the output of the convolutional layers. This is similar to the FastText model where we performed the average over each of the word vectors, implemented by the `F.avg_pool2d` function, however instead of taking the average over a dimension, we are taking the maximum value over a dimension. Below an example of taking the maximum value (0.9) from the output of the convolutional layer on the example sentence (not shown is the activation function applied to the output of the convolutions).\n",
    "\n",
    "![](assets/sentiment15.png)\n",
    "\n",
    "The idea here is that the maximum value is the \"most important\" feature for determining the sentiment of the review, which corresponds to the \"most important\" n-gram within the review. How do we know what the \"most important\" n-gram is? Luckily, we don't have to! Through backpropagation, the weights of the filters are changed so that whenever certain n-grams that are highly indicative of the sentiment are seen, the output of the filter is a \"high\" value. This \"high\" value then passes through the max pooling layer if it is the maximum value in the output. \n",
    "\n",
    "As our model has 100 filters of 3 different sizes, that means we have 300 different n-grams the model thinks are important. We concatenate these together into a single vector and pass them through a linear layer to predict the sentiment. We can think of the weights of this linear layer as \"weighting up the evidence\" from each of the 300 n-grams and making a final decision. \n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "We implement the convolutional layers with `nn.Conv2d`. The `in_channels` argument is the number of \"channels\" in your image going into the convolutional layer. In actual images this is usually 3 (one channel for each of the red, blue and green channels), however when using text we only have a single channel, the text itself. The `out_channels` is the number of filters and the `kernel_size` is the size of the filters. Each of our `kernel_size`s is going to be **[n x emb_dim]** where $n$ is the size of the n-grams.\n",
    "\n",
    "In PyTorch, RNNs want the input with the batch dimension second, whereas CNNs want the batch dimension first - we do not have to permute the data here as we have already set `batch_first = True` in our `TEXT` field. We then pass the sentence through an embedding layer to get our embeddings. The second dimension of the input into a `nn.Conv2d` layer must be the channel dimension. As text technically does not have a channel dimension, we `unsqueeze` our tensor to create one. This matches with our `in_channels=1` in the initialization of our convolutional layers. \n",
    "\n",
    "We then pass the tensors through the convolutional and pooling layers, using the `ReLU` activation function after the convolutional layers. Another nice feature of the pooling layers is that they handle sentences of different lengths. The size of the output of the convolutional layer is dependent on the size of the input to it, and different batches contain sentences of different lengths. Without the max pooling layer the input to our linear layer would depend on the size of the input sentence (not what we want). One option to rectify this would be to trim/pad all sentences to the same length, however with the max pooling layer we always know the input to the linear layer will be the total number of filters. **Note**: there an exception to this if your sentence(s) are shorter than the largest filter used. You will then have to pad your sentences to the length of the largest filter. In the IMDb data there are no reviews only 5 words long so we don't have to worry about that, but you will if you are using your own data.\n",
    "\n",
    "Finally, we perform dropout on the concatenated filter outputs and then pass them through a linear layer to make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = CNN( # CNN1d if we want to run the 1-dimensional convolutional model, noting that both models give almost identical results.\n",
    "        vocab_size = 1, # will be update during compilation to len(TEXT.vocab) during compilation\n",
    "        embedding_dim = 100, \n",
    "        n_filters = 100, \n",
    "        filter_sizes = [3,4,5], \n",
    "        output_dim = 1,  \n",
    "        dropout = 0.5, \n",
    "        pad_idx = 1 # will be update during compilation to TEXT.vocab.stoi[TEXT.pad_token]\n",
    "    ), \n",
    "    dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data, add optimizer and criterion (loss function)\n",
    "Unlike the previous notebook with the FastText model, we no longer explicitly need to create the bi-grams and append them to the end of the sentence.\n",
    "\n",
    "As convolutional layers expect the batch dimension to be first we can tell TorchText to return the data already permuted using the `batch_first = True` argument on the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.compile(\n",
    "    optimizer = \"Adam\", # algorithm we use to update the parameters of the module\n",
    "    criterion = \"BCEWithLogitsLoss\", # loss function\n",
    "    seed = 1234, # random seeds for reproducibility\n",
    "    split_ratio = 0.8, # ratio of training data to use for training, the rest for validation\n",
    "    batch_size = 4, # batch size\n",
    "    max_vocab_size = 25000 # maximun token in the vocabulary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    max_epochs = 50, # maximun number of epochs\n",
    "    improving_limit = 2, # If the precision of the model does not improve during `improving_limit` epoch, we stop training and keep the best model.\n",
    "    dump_id = \"\" # identifier to distinguish models in the serialization folder, is by default equal to the name of the base model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test the model : plot the loss, the accuracy and the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(dump_id = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, as a sanity check we can check some input sentences\n",
    "\n",
    "Note: As mentioned in the implementation details, the input sentence has to be at least as long as the largest filter height used. We modify our predict_sentiment function to also accept a minimum length argument. If the tokenized input sentence is less than min_len tokens, we append padding tokens (<pad>) to make it min_len tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example negative review...\n",
    "print(predict(sentence = \"What can you do with the fire ?\"))\n",
    "# example positive review...\n",
    "print(predict(sentence = \"What can you do with the fire ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    # create an instance of our model and trainer using standard hyperparameters.\n",
    "    model = BERTGRUSentiment(\n",
    "        bert = BertModel.from_pretrained('bert-base-uncased'), # load the pre-trained model, making sure to load the same model as we will do for the tokenizer.\n",
    "        hidden_dim = 256, \n",
    "        output_dim = 1, \n",
    "        n_layers = 2, \n",
    "        bidirectional = True, \n",
    "        dropout = 0.25\n",
    "    ),\n",
    "    dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data, add optimizer and criterion (loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.compile(optimizer = \"Adam\", criterion = \"BCEWithLogitsLoss\", seed = 1234, batch_size = 64)\n",
    "\n",
    "trainer.compile(\n",
    "    optimizer = \"Adam\", # algorithm we use to update the parameters of the module\n",
    "    criterion = \"BCEWithLogitsLoss\", # loss function\n",
    "    seed = 1234, # random seeds for reproducibility\n",
    "    split_ratio = 0.8, # ratio of training data to use for training, the rest for validation\n",
    "    batch_size = 4 # batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to freeze paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in trainer.model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "nb_p = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "print(f'The model has {nb_p:,} trainable parameters ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check the names of the trainable parameters, ensuring they make sense. As we can see, they are all the parameters \n",
    "of the GRU (`rnn`) and the linear layer (`out`).            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in trainer.model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bert `tokenizer` has a `vocab` attribute which contains the actual vocabulary we will be using. We can check how many tokens are in it by checking its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainer.tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tokenizer is as simple as calling `tokenizer.tokenize` on a string. This will tokenize and lower case the data in a way that is consistent with the pre-trained transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = trainer.tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can numericalize tokens using our vocabulary using `tokenizer.convert_tokens_to_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = trainer.tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer was also trained with special tokens to mark the beginning and end of the sentence. As well as a standard padding and unknown token. We can also get these from the tokenizer.\n",
    "\n",
    "**Note**: the tokenizer does have a beginning of sequence and end of sequence attributes (`bos_token` and `eos_token`) but these are not set and should not be used for this transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_token = trainer.tokenizer.cls_token\n",
    "eos_token = trainer.tokenizer.sep_token\n",
    "pad_token = trainer.tokenizer.pad_token\n",
    "unk_token = trainer.tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the indexes of the special tokens by converting them using the vocabulary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_token_idx = trainer.tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = trainer.tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = trainer.tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = trainer.tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or by explicitly getting them from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_token_idx = trainer.tokenizer.cls_token_id\n",
    "eos_token_idx = trainer.tokenizer.sep_token_id\n",
    "pad_token_idx = trainer.tokenizer.pad_token_id\n",
    "unk_token_idx = trainer.tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check an example and ensure that the text has already been numericalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(trainer.dataset[\"train_data\"].examples[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `convert_ids_to_tokens` to transform these indexes back into readable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = trainer.tokenizer.convert_ids_to_tokens(vars(trainer.dataset[\"train_data\"].examples[6])['text'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "This takes considerably longer than any of the previous models due to the size of the transformer. Even though we are not training any of the transformer's parameters we still need to pass the data through the model which takes a considerable amount of time on a standard GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    max_epochs = 50, # maximun number of epochs\n",
    "    improving_limit = 2, # If the precision of the model does not improve during `improving_limit` epoch, we stop training and keep the best model.\n",
    "    dump_id = \"\" # identifier to distinguish models in the serialization folder, is by default equal to the name of the base model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test the model : plot the loss, the accuracy and the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(dump_id = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "We'll then use the model to test the sentiment of some sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example negative review...\n",
    "print(predict(sentence = \"What can you do with the fire ?\"))\n",
    "# example positive review...\n",
    "print(predict(sentence = \"What can you do with the fire ?\"))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
