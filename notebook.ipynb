{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import get_dataset, Trainer, RNN, LSTM, CNN, CNN1d, BERTGRUSentiment\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 1 # len(dataset[\"TEXT\"].vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n",
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "The model has 2,592,105 trainable parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.compile(optimizer = \"SGD\", criterion = \"BCEWithLogitsLoss\", seed = 1234, batch_size = 64, max_vocab_size = 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['How', 'can', 'a', 'movie', 'with', 'Amy', ',', 'Posey', 'and', 'Raechel', 'have', 'NOTHING', 'funny', 'in', 'it', '?', 'Believe', 'it', 'or', 'not', \"'\", 'House', 'Bunny', \"'\", 'did', 'this', 'better', 'and', 'funnier', '.', 'Hopefully', 'the', 'principals', 'had', 'a', 'good', 'holiday', 'and', 'got', 'some', 'money', '-', 'this', 'movie', 'is', 'an', 'embarrassment', 'to', 'all', 'of', 'them', '.', 'It', 'is', 'a', 'cliché', 'from', 'beginning', 'to', 'end', '.', 'Clichés', 'can', 'work', 'well', 'with', 'a', 'script', ',', 'or', 'at', 'least', 'an', 'idea', '.', 'This', 'movie', 'does', 'nothing', 'but', 'use', 'cliché', 'after', 'cliché', 'rather', 'than', 'ideas', 'or', 'script', '.', 'It', 'uses', 'the', 'preexisting', 'persona', \"'s\", 'of', 'the', 'actresses', 'rather', 'than', 'develop', 'characters', '.', 'Bad', ',', 'sad', ',', 'and', 'rubbish', '.', 'Now', 'I', 'apparently', 'have', 'to', 'have', 'ten', 'lines', 'of', 'text', 'for', 'a', 'comment', '.', 'Really', '?', 'Why', '?', 'As', 'an', 'IT', 'ops', 'manager', 'this', 'is', 'another', 'example', 'of', 'sloppy', 'coding', '.'], 'label': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(trainer.dataset[\"train_data\"].examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the vocab size 25002 and not 25000? One of the addition tokens is the `<unk>` token and the other is a `<pad>` token.\n",
    "\n",
    "When we feed sentences into our model, we feed a _batch_ of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size. Thus, to ensure each sentence in the batch is the same size, any shorter than the longest within the batch are padded.\n",
    "\n",
    "We can also view the most common words in the vocabulary and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 202820), (',', 192918), ('.', 164754), ('and', 109435), ('a', 109381), ('of', 101102), ('to', 93825), ('is', 76552), ('in', 61403), ('I', 54228), ('it', 53519), ('that', 49356), ('\"', 44224), (\"'s\", 43374), ('this', 42438), ('-', 37284), ('/><br', 35417), ('was', 34920), ('as', 30245), ('with', 29869)]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.dataset[\"TEXT\"].vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the vocabulary directly using either the `stoi` (string to int) or `itos` (int to string) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(trainer.dataset[\"TEXT\"].vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the labels, ensuring 0 is for negative and 1 is for positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(trainer.dataset[\"LABEL\"].vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 35s\n",
      "\tTrain Loss: 0.708 | Train Acc: 49.45%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 50.36%\n",
      "\tNew best validation score\n",
      "Epoch: 02 | Epoch Time: 0m 35s\n",
      "\tTrain Loss: 0.696 | Train Acc: 49.69%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 50.60%\n",
      "\tNew best validation score\n",
      "Epoch: 03 | Epoch Time: 0m 36s\n",
      "\tTrain Loss: 0.696 | Train Acc: 49.42%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 50.44%\n",
      "\tNot a better validation score (0 / 2).\n",
      "Epoch: 04 | Epoch Time: 0m 35s\n",
      "\tTrain Loss: 0.696 | Train Acc: 49.90%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 51.12%\n",
      "\tNew best validation score\n",
      "Epoch: 05 | Epoch Time: 0m 35s\n",
      "\tTrain Loss: 0.696 | Train Acc: 50.05%\n",
      "\t Val. Loss: 0.701 |  Val. Acc: 49.74%\n",
      "\tNot a better validation score (0 / 2).\n",
      "Epoch: 06 | Epoch Time: 0m 36s\n",
      "\tTrain Loss: 0.696 | Train Acc: 50.41%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 50.79%\n",
      "\tNot a better validation score (1 / 2).\n",
      "Epoch: 07 | Epoch Time: 0m 35s\n",
      "\tTrain Loss: 0.696 | Train Acc: 49.78%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.62%\n",
      "\tNot a better validation score (2 / 2).\n"
     ]
    }
   ],
   "source": [
    "trainer.train(max_epochs = 50, improving_limit = 2, dump_id = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.689 | Test Acc: 54.04%\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() #.predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8343528509140015\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"What can you do with the fire ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5052047967910767\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"NLP is awesome !\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8909756541252136\n",
      "0.19438283145427704\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"Corona-virus has killed the whole world, the economy is suffering, empires have fallen, what a disaster !\"))\n",
    "print(predict(\"Gone has killed the whole world, the economy is suffering, empires have fallen, what a disaster !\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 25002 # will be update to len(dataset[\"TEXT\"].vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = 1 # will be update to dataset[\"TEXT\"].vocab.stoi[dataset[\"TEXT\"].pad_token]\n",
    "\n",
    "model = LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n",
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "pretrained_embeddings.shape torch.Size([25002, 100])\n",
      "self.model.embedding.weight.data tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.2295, -0.4421,  0.4331,  ..., -0.3084, -0.3175,  0.3562],\n",
      "        [-0.0046, -0.4325, -0.3023,  ...,  0.3291, -0.1993, -0.2312],\n",
      "        [-0.0212,  0.5229,  0.7983,  ..., -0.4114,  0.0744,  0.6785]],\n",
      "       device='cuda:0')\n",
      "The model has 4,810,857 trainable parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#trainer.compile(optimizer = \"Adam\", criterion = \"BCEWithLogitsLoss\", seed = 1234, batch_size = 64, max_vocab_size = 25000)\n",
    "trainer.compile(optimizer = \"Adam\", criterion = \"BCEWithLogitsLoss\", seed = 1234, batch_size = 1, max_vocab_size = 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 29m 47s\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.58%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 55.29%\n",
      "\tNew best validation score\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-778479ef5636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimproving_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/wl_research/wl_research_challenge/src/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_epochs, improving_limit, dump_id)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m#if criterion == \"BCEWithLogitsLoss\" :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wl_research/wl_research_challenge/src/model.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mLuckily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIn\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mare\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mcasing\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mlower\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mevery\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mWe\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mby\u001b[0m \u001b[0mloading\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrained\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0muncased\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \"\"\"\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(max_epochs = 50, improving_limit = 2, dump_id = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() #.predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49243149161338806\n"
     ]
    }
   ],
   "source": [
    "print(predict(sentence = \"What can you do with the fire ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM =   100 # will be update to len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = 1 # will be update to TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n",
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "The model has 2,620,801 trainable parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.compile(optimizer = \"Adam\", criterion = \"BCEWithLogitsLoss\", seed = 1234, batch_size = 64, max_vocab_size = 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.729 | Train Acc: 49.66%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.26%\n",
      "\tNew best validation score\n",
      "Epoch: 02 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.722 | Train Acc: 50.40%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.57%\n",
      "\tNew best validation score\n",
      "Epoch: 03 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.719 | Train Acc: 50.33%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 52.64%\n",
      "\tNew best validation score\n",
      "Epoch: 04 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.715 | Train Acc: 50.72%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 53.66%\n",
      "\tNew best validation score\n",
      "Epoch: 05 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.711 | Train Acc: 50.44%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 54.38%\n",
      "\tNew best validation score\n",
      "Epoch: 06 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.704 | Train Acc: 51.82%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 54.02%\n",
      "\tNot a better validation score (0 / 2).\n",
      "Epoch: 07 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.706 | Train Acc: 50.83%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 55.23%\n",
      "\tNew best validation score\n",
      "Epoch: 08 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.705 | Train Acc: 50.94%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 56.69%\n",
      "\tNew best validation score\n",
      "Epoch: 09 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.700 | Train Acc: 52.13%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 51.89%\n",
      "\tNot a better validation score (0 / 2).\n",
      "Epoch: 10 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.697 | Train Acc: 52.07%\n",
      "\t Val. Loss: 0.686 |  Val. Acc: 56.61%\n",
      "\tNew best validation score\n",
      "Epoch: 11 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.697 | Train Acc: 51.93%\n",
      "\t Val. Loss: 0.686 |  Val. Acc: 57.41%\n",
      "\tNot a better validation score (0 / 2).\n",
      "Epoch: 12 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.695 | Train Acc: 52.85%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 55.81%\n",
      "\tNot a better validation score (1 / 2).\n",
      "Epoch: 13 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.693 | Train Acc: 52.78%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 57.79%\n",
      "\tNew best validation score\n",
      "Epoch: 14 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.694 | Train Acc: 52.56%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 56.89%\n",
      "\tNew best validation score\n",
      "Epoch: 15 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.692 | Train Acc: 53.09%\n",
      "\t Val. Loss: 0.684 |  Val. Acc: 58.20%\n",
      "\tNew best validation score\n",
      "Epoch: 16 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.692 | Train Acc: 53.03%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 57.04%\n",
      "\tNot a better validation score (0 / 2).\n",
      "Epoch: 17 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.690 | Train Acc: 53.47%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 58.42%\n",
      "\tNot a better validation score (1 / 2).\n",
      "Epoch: 18 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.690 | Train Acc: 53.20%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 56.12%\n",
      "\tNot a better validation score (2 / 2).\n"
     ]
    }
   ],
   "source": [
    "trainer.train(max_epochs = 50, improving_limit = 2, dump_id = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.748 | Test Acc: 49.99%\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() #.predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.304843932390213\n"
     ]
    }
   ],
   "source": [
    "print(predict(sentence = \"What can you do with the fire ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "model = BERTGRUSentiment(bert, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 112,241,409 total parameters\n",
      "The model has 2,759,169 trainable parameters ...\n",
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, dump_path=\"/home/jupyter/wl_research/wl_research_challenge/dump_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "trainer.compile(optimizer = \"Adam\", criterion = \"BCEWithLogitsLoss\", seed = 1234, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-778479ef5636>\", line 1, in <module>\n",
      "    trainer.train(max_epochs = 50, improving_limit = 2, dump_id = \"\")\n",
      "  File \"/home/jupyter/wl_research/wl_research_challenge/src/model.py\", line 598, in train\n",
      "    train_loss, train_acc = self.train_step(self.dataset[\"train_iterator\"])\n",
      "  File \"/home/jupyter/wl_research/wl_research_challenge/src/model.py\", line 536, in train_step\n",
      "    predictions = self.model(batch.text).squeeze(1)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/jupyter/wl_research/wl_research_challenge/src/model.py\", line 253, in forward\n",
      "    _, hidden = self.rnn(embedded)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\", line 716, in forward\n",
      "    self.dropout, self.training, self.bidirectional, self.batch_first)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/opt/conda/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/opt/conda/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/opt/conda/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "trainer.train(max_epochs = 50, improving_limit = 2, dump_id = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.692 | Test Acc: 52.21%\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = trainer.get_predict_sentiment() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(sentence = \"What can you do with the fire ?\"))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
